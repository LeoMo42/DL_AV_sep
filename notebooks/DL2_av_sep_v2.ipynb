{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL2_av_sep.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wl5aZThEu84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f6426f07-b63f-4e9c-c9f6-be3b8a4a1c6c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP9J4iGoE53n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q ./gdrive/My\\ Drive/DL2/DL_AV_sep/DL_AV_sep.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC1Q6X1yPdKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q ./gdrive/My\\ Drive/DL2/DL_AV_sep/data.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsrcCFJgFI23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0a12af5-a3df-408a-cc63-375c1e0e4acb"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.6.3)\n",
            "Requirement already satisfied: numba==0.48 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.48.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.0.5)\n",
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (2.3.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (7.0.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (0.22.2.post1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (2.1.8)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 1)) (0.15.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba==0.48->-r requirements.txt (line 2)) (47.3.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba==0.48->-r requirements.txt (line 2)) (0.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn->-r requirements.txt (line 4)) (4.1.2.30)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.30.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.2.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.34.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 7)) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 7)) (1.0.8)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.6.0.post3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (0.4.8)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV2ZCjPxFQVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04ff14e5-bb01-46ce-818d-ab97bd9b7cce"
      },
      "source": [
        "!python setup.py develop"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running develop\n",
            "running egg_info\n",
            "creating av_sep.egg-info\n",
            "writing av_sep.egg-info/PKG-INFO\n",
            "writing dependency_links to av_sep.egg-info/dependency_links.txt\n",
            "writing requirements to av_sep.egg-info/requires.txt\n",
            "writing top-level names to av_sep.egg-info/top_level.txt\n",
            "writing manifest file 'av_sep.egg-info/SOURCES.txt'\n",
            "reading manifest file 'av_sep.egg-info/SOURCES.txt'\n",
            "writing manifest file 'av_sep.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.6/dist-packages/av-sep.egg-link (link to .)\n",
            "Adding av-sep 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content\n",
            "Processing dependencies for av-sep==0.0.1\n",
            "Searching for Pillow==7.0.0\n",
            "Best match: Pillow 7.0.0\n",
            "Adding Pillow 7.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras==2.3.1\n",
            "Best match: Keras 2.3.1\n",
            "Adding Keras 2.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for matplotlib==3.2.2\n",
            "Best match: matplotlib 3.2.2\n",
            "Adding matplotlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorflow==2.2.0\n",
            "Best match: tensorflow 2.2.0\n",
            "Adding tensorflow 2.2.0 to easy-install.pth file\n",
            "Installing estimator_ckpt_converter script to /usr/local/bin\n",
            "Installing saved_model_cli script to /usr/local/bin\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "Installing tf_upgrade_v2 script to /usr/local/bin\n",
            "Installing tflite_convert script to /usr/local/bin\n",
            "Installing toco script to /usr/local/bin\n",
            "Installing toco_from_protos script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for mtcnn==0.1.0\n",
            "Best match: mtcnn 0.1.0\n",
            "Adding mtcnn 0.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pandas==1.0.5\n",
            "Best match: pandas 1.0.5\n",
            "Adding pandas 1.0.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numba==0.48.0\n",
            "Best match: numba 0.48.0\n",
            "Adding numba 0.48.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for librosa==0.6.3\n",
            "Best match: librosa 0.6.3\n",
            "Adding librosa 0.6.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Applications==1.0.8\n",
            "Best match: Keras-Applications 1.0.8\n",
            "Adding Keras-Applications 1.0.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.5\n",
            "Best match: numpy 1.18.5\n",
            "Adding numpy 1.18.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.1.2\n",
            "Best match: Keras-Preprocessing 1.1.2\n",
            "Adding Keras-Preprocessing 1.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.10.0\n",
            "Best match: h5py 2.10.0\n",
            "Adding h5py 2.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.2.0\n",
            "Best match: kiwisolver 1.2.0\n",
            "Adding kiwisolver 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.7\n",
            "Best match: pyparsing 2.4.7\n",
            "Adding pyparsing 2.4.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for gast==0.3.3\n",
            "Best match: gast 0.3.3\n",
            "Adding gast 0.3.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorflow-estimator==2.2.0\n",
            "Best match: tensorflow-estimator 2.2.0\n",
            "Adding tensorflow-estimator 2.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard==2.2.2\n",
            "Best match: tensorboard 2.2.2\n",
            "Adding tensorboard 2.2.2 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wrapt==1.12.1\n",
            "Best match: wrapt 1.12.1\n",
            "Adding wrapt 1.12.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for opt-einsum==3.2.1\n",
            "Best match: opt-einsum 3.2.1\n",
            "Adding opt-einsum 3.2.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for astunparse==1.6.3\n",
            "Best match: astunparse 1.6.3\n",
            "Adding astunparse 1.6.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wheel==0.34.2\n",
            "Best match: wheel 0.34.2\n",
            "Adding wheel 0.34.2 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for grpcio==1.30.0\n",
            "Best match: grpcio 1.30.0\n",
            "Adding grpcio 1.30.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for protobuf==3.10.0\n",
            "Best match: protobuf 3.10.0\n",
            "Adding protobuf 3.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for absl-py==0.9.0\n",
            "Best match: absl-py 0.9.0\n",
            "Adding absl-py 0.9.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-pasta==0.2.0\n",
            "Best match: google-pasta 0.2.0\n",
            "Adding google-pasta 0.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for opencv-python==4.1.2.30\n",
            "Best match: opencv-python 4.1.2.30\n",
            "Adding opencv-python 4.1.2.30 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for llvmlite==0.31.0\n",
            "Best match: llvmlite 0.31.0\n",
            "Adding llvmlite 0.31.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==47.3.1\n",
            "Best match: setuptools 47.3.1\n",
            "Adding setuptools 47.3.1 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for audioread==2.1.8\n",
            "Best match: audioread 2.1.8\n",
            "Adding audioread 2.1.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for decorator==4.4.2\n",
            "Best match: decorator 4.4.2\n",
            "Adding decorator 4.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for resampy==0.2.2\n",
            "Best match: resampy 0.2.2\n",
            "Adding resampy 0.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for joblib==0.15.1\n",
            "Best match: joblib 0.15.1\n",
            "Adding joblib 0.15.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth==1.17.2\n",
            "Best match: google-auth 1.17.2\n",
            "Adding google-auth 1.17.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Markdown==3.2.2\n",
            "Best match: Markdown 3.2.2\n",
            "Adding Markdown 3.2.2 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard-plugin-wit==1.6.0.post3\n",
            "Best match: tensorboard-plugin-wit 1.6.0.post3\n",
            "Adding tensorboard-plugin-wit 1.6.0.post3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.1\n",
            "Best match: google-auth-oauthlib 0.4.1\n",
            "Adding google-auth-oauthlib 0.4.1 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cachetools==4.1.0\n",
            "Best match: cachetools 4.1.0\n",
            "Adding cachetools 4.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for rsa==4.6\n",
            "Best match: rsa 4.6\n",
            "Adding rsa 4.6 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==1.6.1\n",
            "Best match: importlib-metadata 1.6.1\n",
            "Adding importlib-metadata 1.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for certifi==2020.6.20\n",
            "Best match: certifi 2020.6.20\n",
            "Adding certifi 2020.6.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for idna==2.9\n",
            "Best match: idna 2.9\n",
            "Adding idna 2.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests-oauthlib==1.3.0\n",
            "Best match: requests-oauthlib 1.3.0\n",
            "Adding requests-oauthlib 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.1.0\n",
            "Best match: zipp 3.1.0\n",
            "Adding zipp 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for oauthlib==3.1.0\n",
            "Best match: oauthlib 3.1.0\n",
            "Adding oauthlib 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for av-sep==0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgr04LqXF1q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/questions/52142671/google-colab-redirect-python-output-to-a-log-file\n",
        "!python tree_text_gen/binary/translation/train.py --expr-name baseline_20200525-3 --datadir ./iwslt/IWSLT/en-de/ --model-type translation --beta-burnin 2 --beta-step 0.05 --self-teach-beta-step 0.05 --log-base-dir gdrive/My\\ Drive/RL_proj/output/checkpoints 2>&1 | tee translation_new_oracle_logs.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGbe2Gz9QXOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d5ff733-5515-46bb-f967-3164a29a35cd"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkNCyDPuQo__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9006bbc2-4b93-46d0-e077-c098d65f1e46"
      },
      "source": [
        "from av_sep.models.model_v2.AV_train import AvTrain\n",
        "AvTrain.run(\"/content/data/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "as_0: (None, 298, 257, 2)\n",
            "as_1: (None, 298, 257, 96)\n",
            "as_2: (None, 298, 257, 96)\n",
            "as_3: (None, 298, 257, 96)\n",
            "as_4: (None, 298, 257, 96)\n",
            "as_5: (None, 298, 257, 96)\n",
            "as_6: (None, 298, 257, 96)\n",
            "as_7: (None, 298, 257, 96)\n",
            "as_8: (None, 298, 257, 96)\n",
            "as_9: (None, 298, 257, 96)\n",
            "as_10: (None, 298, 257, 96)\n",
            "as_11: (None, 298, 257, 96)\n",
            "as_12: (None, 298, 257, 96)\n",
            "as_13: (None, 298, 257, 96)\n",
            "as_14: (None, 298, 257, 96)\n",
            "as_15: (None, 298, 257, 8)\n",
            "AS_out: (None, 298, 2056)\n",
            "AVfusion: (None, 298, 2568)\n",
            "lstm: (None, 298, 400)\n",
            "fc1: (None, 298, 600)\n",
            "fc2: (None, 298, 600)\n",
            "fc3: (None, 298, 600)\n",
            "complex_mask: (None, 298, 1028)\n",
            "complex_mask_out: (None, 298, 257, 2, 2)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 298, 257, 2)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "as_conv1 (Conv2D)               (None, 298, 257, 96) 1440        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 298, 257, 96) 384         as_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv2 (Conv2D)               (None, 298, 257, 96) 64608       re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 298, 257, 96) 384         as_conv2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_2 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv3 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 298, 257, 96) 384         as_conv3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv4 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 298, 257, 96) 384         as_conv4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_4 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv5 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 298, 257, 96) 384         as_conv5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_5 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv6 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 298, 257, 96) 384         as_conv6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_6 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv7 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 298, 257, 96) 384         as_conv7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_7 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv8 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 298, 257, 96) 384         as_conv8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_8 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv9 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 298, 257, 96) 384         as_conv9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_9 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv10 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 298, 257, 96) 384         as_conv10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_10 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv11 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 298, 257, 96) 384         as_conv11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_11 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv12 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 298, 257, 96) 384         as_conv12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv13 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 298, 257, 96) 384         as_conv13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_13 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv14 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 298, 257, 96) 384         as_conv14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_14 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv15 (Conv2D)              (None, 298, 257, 8)  776         re_lu_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 298, 257, 8)  32          as_conv15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 75, 1, 1792,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_15 (ReLU)                 (None, 298, 257, 8)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 75, 1, 1792)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 75, 1, 1792)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 298, 2056)    0           re_lu_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 298, 256)     4857344     lambda_2[0][0]                   \n",
            "                                                                 lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 298, 2568)    0           reshape_1[0][0]                  \n",
            "                                                                 sequential_1[1][0]               \n",
            "                                                                 sequential_1[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 298, 2568)    0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 298, 400)     9500800     time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "fc1 (Dense)                     (None, 298, 600)     240600      bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "fc2 (Dense)                     (None, 298, 600)     360600      fc1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "fc3 (Dense)                     (None, 298, 600)     360600      fc2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "complex_mask (Dense)            (None, 298, 1028)    617828      fc3[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 298, 257, 2,  0           complex_mask[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 18,775,956\n",
            "Trainable params: 18,770,180\n",
            "Non-trainable params: 5,776\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 544s 2s/step - loss: 0.7056 - val_loss: 0.5599\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.55989, saving model to /content/data/saved_AV_models/AVmodel-2p-001-0.55989.h5\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 527s 2s/step - loss: 0.6005 - val_loss: 0.5635\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.55989\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 526s 2s/step - loss: 0.5970 - val_loss: 0.6461\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.55989\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 527s 2s/step - loss: 0.5839 - val_loss: 0.3833\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.55989 to 0.38334, saving model to /content/data/saved_AV_models/AVmodel-2p-004-0.38334.h5\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 527s 2s/step - loss: 0.5390 - val_loss: 0.4319\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.38334\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 525s 2s/step - loss: 0.5161 - val_loss: 0.5472\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.38334\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 523s 2s/step - loss: 0.5103 - val_loss: 0.7090\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.38334\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 502s 2s/step - loss: 0.5082 - val_loss: 0.6974\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.38334\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.5044 - val_loss: 0.5518\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.38334\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 492s 2s/step - loss: 0.5029 - val_loss: 0.6276\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.38334\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.5016 - val_loss: 0.5510\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.38334\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 490s 1s/step - loss: 0.4997 - val_loss: 0.3517\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.38334 to 0.35168, saving model to /content/data/saved_AV_models/AVmodel-2p-012-0.35168.h5\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4992 - val_loss: 0.4674\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.35168\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4985 - val_loss: 0.4962\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.35168\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 490s 1s/step - loss: 0.4980 - val_loss: 0.5514\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.35168\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.4979 - val_loss: 0.4803\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.35168\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 489s 1s/step - loss: 0.4967 - val_loss: 0.6276\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.35168\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.4964 - val_loss: 0.5927\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.35168\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.4954 - val_loss: 0.4363\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.35168\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4949 - val_loss: 0.5956\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.35168\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 490s 1s/step - loss: 0.4944 - val_loss: 0.6736\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.35168\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 494s 2s/step - loss: 0.4937 - val_loss: 0.4181\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.35168\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 503s 2s/step - loss: 0.4931 - val_loss: 0.6439\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.35168\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 507s 2s/step - loss: 0.4924 - val_loss: 0.4405\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.35168\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 508s 2s/step - loss: 0.4924 - val_loss: 0.5416\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.35168\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 505s 2s/step - loss: 0.4917 - val_loss: 0.4893\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.35168\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 509s 2s/step - loss: 0.4914 - val_loss: 0.6171\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.35168\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 509s 2s/step - loss: 0.4908 - val_loss: 0.5063\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.35168\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4903 - val_loss: 0.6329\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.35168\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 512s 2s/step - loss: 0.4899 - val_loss: 0.5211\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.35168\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4893 - val_loss: 0.3816\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.35168\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4890 - val_loss: 0.4444\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.35168\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4883 - val_loss: 0.5211\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.35168\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 511s 2s/step - loss: 0.4881 - val_loss: 0.7142\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.35168\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 511s 2s/step - loss: 0.4875 - val_loss: 0.5173\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.35168\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 522s 2s/step - loss: 0.4872 - val_loss: 0.3863\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.35168\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 511s 2s/step - loss: 0.4867 - val_loss: 0.6178\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.35168\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 512s 2s/step - loss: 0.4861 - val_loss: 0.4377\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.35168\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 513s 2s/step - loss: 0.4857 - val_loss: 0.4201\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.35168\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 517s 2s/step - loss: 0.4848 - val_loss: 0.5181\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.35168\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 501s 2s/step - loss: 0.4843 - val_loss: 0.4814\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.35168\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 513s 2s/step - loss: 0.4838 - val_loss: 0.2832\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.35168 to 0.28321, saving model to /content/data/saved_AV_models/AVmodel-2p-042-0.28321.h5\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 513s 2s/step - loss: 0.4831 - val_loss: 0.5368\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.28321\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 513s 2s/step - loss: 0.4824 - val_loss: 0.4772\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.28321\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 512s 2s/step - loss: 0.4818 - val_loss: 0.6839\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.28321\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 512s 2s/step - loss: 0.4809 - val_loss: 0.4310\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.28321\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4803 - val_loss: 0.4046\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.28321\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 515s 2s/step - loss: 0.4794 - val_loss: 0.6019\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.28321\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 515s 2s/step - loss: 0.4788 - val_loss: 0.3487\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.28321\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 506s 2s/step - loss: 0.4777 - val_loss: 0.5918\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.28321\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 504s 2s/step - loss: 0.4768 - val_loss: 0.5034\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.28321\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 505s 2s/step - loss: 0.4759 - val_loss: 0.4817\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.28321\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 505s 2s/step - loss: 0.4751 - val_loss: 0.6606\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.28321\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 501s 2s/step - loss: 0.4741 - val_loss: 0.5187\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.28321\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 500s 2s/step - loss: 0.4732 - val_loss: 0.6115\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.28321\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 499s 2s/step - loss: 0.4724 - val_loss: 0.5582\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.28321\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 504s 2s/step - loss: 0.4714 - val_loss: 0.6287\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.28321\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 501s 2s/step - loss: 0.4706 - val_loss: 0.4182\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.28321\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 502s 2s/step - loss: 0.4698 - val_loss: 0.6547\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.28321\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 504s 2s/step - loss: 0.4688 - val_loss: 0.5269\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.28321\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 506s 2s/step - loss: 0.4682 - val_loss: 0.4784\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.28321\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 506s 2s/step - loss: 0.4673 - val_loss: 0.3266\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.28321\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 503s 2s/step - loss: 0.4665 - val_loss: 0.6082\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.28321\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 506s 2s/step - loss: 0.4660 - val_loss: 0.3897\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.28321\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 502s 2s/step - loss: 0.4652 - val_loss: 0.4511\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.28321\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 503s 2s/step - loss: 0.4646 - val_loss: 0.6249\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.28321\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 506s 2s/step - loss: 0.4640 - val_loss: 0.2229\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.28321 to 0.22288, saving model to /content/data/saved_AV_models/AVmodel-2p-067-0.22288.h5\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 505s 2s/step - loss: 0.4634 - val_loss: 0.4781\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.22288\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 508s 2s/step - loss: 0.4629 - val_loss: 0.3867\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.22288\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 501s 2s/step - loss: 0.4624 - val_loss: 0.4005\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.22288\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 501s 2s/step - loss: 0.4619 - val_loss: 0.5350\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.22288\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 500s 2s/step - loss: 0.4613 - val_loss: 0.4476\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.22288\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 495s 2s/step - loss: 0.4609 - val_loss: 0.4167\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.22288\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 490s 1s/step - loss: 0.4603 - val_loss: 0.3274\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.22288\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 490s 1s/step - loss: 0.4599 - val_loss: 0.4417\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.22288\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 517s 2s/step - loss: 0.4596 - val_loss: 0.4225\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.22288\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4591 - val_loss: 0.4978\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.22288\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 501s 2s/step - loss: 0.4588 - val_loss: 0.5071\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.22288\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 489s 1s/step - loss: 0.4584 - val_loss: 0.3959\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.22288\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 492s 1s/step - loss: 0.4579 - val_loss: 0.4117\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.22288\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 488s 1s/step - loss: 0.4577 - val_loss: 0.4661\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.22288\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 485s 1s/step - loss: 0.4574 - val_loss: 0.4110\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.22288\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 488s 1s/step - loss: 0.4571 - val_loss: 0.4778\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.22288\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 501s 2s/step - loss: 0.4567 - val_loss: 0.3265\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.22288\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4565 - val_loss: 0.3064\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.22288\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 518s 2s/step - loss: 0.4561 - val_loss: 0.5188\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.22288\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 522s 2s/step - loss: 0.4559 - val_loss: 0.4740\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.22288\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 517s 2s/step - loss: 0.4556 - val_loss: 0.4920\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.22288\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 515s 2s/step - loss: 0.4553 - val_loss: 0.4002\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.22288\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 508s 2s/step - loss: 0.4550 - val_loss: 0.4330\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.22288\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4549 - val_loss: 0.4222\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.22288\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4546 - val_loss: 0.3768\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.22288\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 489s 1s/step - loss: 0.4542 - val_loss: 0.4737\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.22288\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.4543 - val_loss: 0.2583\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.22288\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.4540 - val_loss: 0.3533\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.22288\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 490s 1s/step - loss: 0.4536 - val_loss: 0.4140\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.22288\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.4534 - val_loss: 0.4548\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.22288\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 500s 2s/step - loss: 0.4533 - val_loss: 0.4294\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.22288\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 508s 2s/step - loss: 0.4530 - val_loss: 0.5476\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.22288\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 510s 2s/step - loss: 0.4528 - val_loss: 0.3499\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.22288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeS15h0Ny_Xe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c18e9649-23dd-4de2-fc95-4c73674ab6bc"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Convolution2D, Bidirectional, concatenate\n",
        "from keras.layers import Flatten, BatchNormalization, ReLU, Reshape, Lambda, TimeDistributed\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.initializers import he_normal, glorot_uniform\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from av_sep.models.lib.MyGenerator import AVGenerator\n",
        "\n",
        "\n",
        "def AV_model(people_num=2):\n",
        "    def UpSampling2DBilinear(size):\n",
        "        return Lambda(lambda x: tf.image.resize(x, size))\n",
        "\n",
        "    def sliced(x, index):\n",
        "        return x[..., index]\n",
        "\n",
        "    # --------------------------- AS start ---------------------------\n",
        "    audio_input = Input(shape=(298, 257, 2))\n",
        "    print('as_0:', audio_input.shape)\n",
        "    as_conv1 = Convolution2D(96, kernel_size=(1, 7), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv1')(audio_input)\n",
        "    as_conv1 = BatchNormalization()(as_conv1)\n",
        "    as_conv1 = ReLU()(as_conv1)\n",
        "    print('as_1:', as_conv1.shape)\n",
        "\n",
        "    as_conv2 = Convolution2D(96, kernel_size=(7, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv2')(as_conv1)\n",
        "    as_conv2 = BatchNormalization()(as_conv2)\n",
        "    as_conv2 = ReLU()(as_conv2)\n",
        "    print('as_2:', as_conv2.shape)\n",
        "\n",
        "    as_conv3 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv3')(as_conv2)\n",
        "    as_conv3 = BatchNormalization()(as_conv3)\n",
        "    as_conv3 = ReLU()(as_conv3)\n",
        "    print('as_3:', as_conv3.shape)\n",
        "\n",
        "    as_conv4 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(2, 1), name='as_conv4')(as_conv3)\n",
        "    as_conv4 = BatchNormalization()(as_conv4)\n",
        "    as_conv4 = ReLU()(as_conv4)\n",
        "    print('as_4:', as_conv4.shape)\n",
        "\n",
        "    as_conv5 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(4, 1), name='as_conv5')(as_conv4)\n",
        "    as_conv5 = BatchNormalization()(as_conv5)\n",
        "    as_conv5 = ReLU()(as_conv5)\n",
        "    print('as_5:', as_conv5.shape)\n",
        "\n",
        "    as_conv6 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(8, 1), name='as_conv6')(as_conv5)\n",
        "    as_conv6 = BatchNormalization()(as_conv6)\n",
        "    as_conv6 = ReLU()(as_conv6)\n",
        "    print('as_6:', as_conv6.shape)\n",
        "\n",
        "    as_conv7 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(16, 1), name='as_conv7')(as_conv6)\n",
        "    as_conv7 = BatchNormalization()(as_conv7)\n",
        "    as_conv7 = ReLU()(as_conv7)\n",
        "    print('as_7:', as_conv7.shape)\n",
        "\n",
        "    as_conv8 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(32, 1), name='as_conv8')(as_conv7)\n",
        "    as_conv8 = BatchNormalization()(as_conv8)\n",
        "    as_conv8 = ReLU()(as_conv8)\n",
        "    print('as_8:', as_conv8.shape)\n",
        "\n",
        "    as_conv9 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv9')(as_conv8)\n",
        "    as_conv9 = BatchNormalization()(as_conv9)\n",
        "    as_conv9 = ReLU()(as_conv9)\n",
        "    print('as_9:', as_conv9.shape)\n",
        "\n",
        "    as_conv10 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(2, 2), name='as_conv10')(as_conv9)\n",
        "    as_conv10 = BatchNormalization()(as_conv10)\n",
        "    as_conv10 = ReLU()(as_conv10)\n",
        "    print('as_10:', as_conv10.shape)\n",
        "\n",
        "    as_conv11 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(4, 4), name='as_conv11')(as_conv10)\n",
        "    as_conv11 = BatchNormalization()(as_conv11)\n",
        "    as_conv11 = ReLU()(as_conv11)\n",
        "    print('as_11:', as_conv11.shape)\n",
        "\n",
        "    as_conv12 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(8, 8), name='as_conv12')(as_conv11)\n",
        "    as_conv12 = BatchNormalization()(as_conv12)\n",
        "    as_conv12 = ReLU()(as_conv12)\n",
        "    print('as_12:', as_conv12.shape)\n",
        "\n",
        "    as_conv13 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(16, 16), name='as_conv13')(as_conv12)\n",
        "    as_conv13 = BatchNormalization()(as_conv13)\n",
        "    as_conv13 = ReLU()(as_conv13)\n",
        "    print('as_13:', as_conv13.shape)\n",
        "\n",
        "    as_conv14 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(32, 32), name='as_conv14')(as_conv13)\n",
        "    as_conv14 = BatchNormalization()(as_conv14)\n",
        "    as_conv14 = ReLU()(as_conv14)\n",
        "    print('as_14:', as_conv14.shape)\n",
        "\n",
        "    as_conv15 = Convolution2D(8, kernel_size=(1, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='as_conv15')(as_conv14)\n",
        "    as_conv15 = BatchNormalization()(as_conv15)\n",
        "    as_conv15 = ReLU()(as_conv15)\n",
        "    print('as_15:', as_conv15.shape)\n",
        "\n",
        "    AS_out = Reshape((298, 8 * 257))(as_conv15)\n",
        "    print('AS_out:', AS_out.shape)\n",
        "    # --------------------------- AS end ---------------------------\n",
        "\n",
        "    # --------------------------- VS_model start ---------------------------\n",
        "    VS_model = Sequential()\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(7, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='vs_conv1'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='vs_conv2'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(2, 1), name='vs_conv3'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(4, 1), name='vs_conv4'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(8, 1), name='vs_conv5'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Convolution2D(256, kernel_size=(5, 1), strides=(1, 1), padding='same', dilation_rate=(16, 1), name='vs_conv6'))\n",
        "    VS_model.add(BatchNormalization())\n",
        "    VS_model.add(ReLU())\n",
        "    VS_model.add(Reshape((75, 256, 1)))\n",
        "    VS_model.add(UpSampling2DBilinear((298, 256)))\n",
        "    VS_model.add(Reshape((298, 256)))\n",
        "    # --------------------------- VS_model end ---------------------------\n",
        "\n",
        "    video_input = Input(shape=(75, 1, 1792, people_num))\n",
        "    AVfusion_list = [AS_out]\n",
        "    for i in range(people_num):\n",
        "        single_input = Lambda(sliced, arguments={'index': i})(video_input)\n",
        "        VS_out = VS_model(single_input)\n",
        "        AVfusion_list.append(VS_out)\n",
        "\n",
        "    AVfusion = concatenate(AVfusion_list, axis=2)\n",
        "    AVfusion = TimeDistributed(Flatten())(AVfusion)\n",
        "    print('AVfusion:', AVfusion.shape)\n",
        "\n",
        "    lstm = Bidirectional(LSTM(1024, input_shape=(298, 8 * 257), return_sequences=True), merge_mode='sum')(AVfusion)\n",
        "    print('lstm:', lstm.shape)\n",
        "\n",
        "    fc1 = Dense(1024, name=\"fc1\", activation='relu', kernel_initializer=he_normal(seed=27))(lstm)\n",
        "    print('fc1:', fc1.shape)\n",
        "    fc2 = Dense(1024, name=\"fc2\", activation='relu', kernel_initializer=he_normal(seed=42))(fc1)\n",
        "    print('fc2:', fc2.shape)\n",
        "    fc3 = Dense(1024, name=\"fc3\", activation='relu', kernel_initializer=he_normal(seed=65))(fc2)\n",
        "    print('fc3:', fc3.shape)\n",
        "\n",
        "    complex_mask = Dense(257 * 2 * people_num, name=\"complex_mask\", kernel_initializer=glorot_uniform(seed=87))(fc3)\n",
        "    print('complex_mask:', complex_mask.shape)\n",
        "\n",
        "    complex_mask_out = Reshape((298, 257, 2, people_num))(complex_mask)\n",
        "    print('complex_mask_out:', complex_mask_out.shape)\n",
        "\n",
        "    AV_model = Model(inputs=[audio_input, video_input], outputs=complex_mask_out)\n",
        "\n",
        "    # # compile AV_model\n",
        "    # AV_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return AV_model\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-uPf3qkjByh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "075f8b32-4a78-4452-8a46-83fe267bccc9"
      },
      "source": [
        "from av_sep.models.lib import model_ops\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import load_model\n",
        "from av_sep.models.lib.MyGenerator import AVGenerator\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras import optimizers\n",
        "import os\n",
        "from av_sep.models.lib.model_loss import audio_discriminate_loss2 as audio_loss\n",
        "import tensorflow as tf\n",
        "\n",
        "ROOTPATH = \"/content/data/\"\n",
        "\n",
        "#############################################################\n",
        "# automatically change lr\n",
        "def scheduler(epoch):\n",
        "    ini_lr = 0.00001\n",
        "    lr = ini_lr\n",
        "    if epoch >= 5:\n",
        "        lr = ini_lr / 5\n",
        "    if epoch >= 10:\n",
        "        lr = ini_lr / 10\n",
        "    return lr\n",
        "\n",
        "# create AV model\n",
        "#############################################################\n",
        "RESTORE = False\n",
        "# If set true, continue training from last checkpoint\n",
        "# needed change 1:h5 file name, 2:epochs num, 3:initial_epoch\n",
        "\n",
        "# super parameters\n",
        "people_num = 2\n",
        "epochs = 50\n",
        "initial_epoch = 0\n",
        "batch_size = 2  # 4 to feed one 16G GPU\n",
        "gamma_loss = 0.1\n",
        "beta_loss = gamma_loss*2\n",
        "\n",
        "# physical devices option to accelerate training process\n",
        "workers = 1 # num of core\n",
        "use_multiprocessing = False\n",
        "NUM_GPU = 1\n",
        "\n",
        "# PATH\n",
        "path = ROOTPATH + 'saved_AV_models'  # model path\n",
        "database_dir_path = ROOTPATH\n",
        "#############################################################\n",
        "\n",
        "# create folder to save models\n",
        "folder = os.path.exists(path)\n",
        "if not folder:\n",
        "    os.makedirs(path)\n",
        "    print('create folder to save models')\n",
        "filepath = path + \"/AVmodel-\" + str(people_num) + \"p-{epoch:03d}-{val_loss:.5f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "\n",
        "rlr = LearningRateScheduler(scheduler, verbose=1)\n",
        "#############################################################\n",
        "# read train and val file name\n",
        "# format: mix.npy single.npy single.npy\n",
        "trainfile = []\n",
        "valfile = []\n",
        "with open((database_dir_path+'audio/AVdataset_train.txt'), 'r') as t:\n",
        "    trainfile = t.readlines()\n",
        "with open((database_dir_path+'audio/AVdataset_val.txt'), 'r') as v:\n",
        "    valfile = v.readlines()\n",
        "# ///////////////////////////////////////////////////////// #\n",
        "\n",
        "# the training steps\n",
        "if RESTORE:\n",
        "    latest_file = model_ops.latest_file(path+'/')\n",
        "    AV_model = load_model(latest_file,custom_objects={\"tf\": tf})\n",
        "    info = latest_file.strip().split('-')\n",
        "    initial_epoch = int(info[-2])\n",
        "else:\n",
        "    AV_model = AV_model(people_num)\n",
        "\n",
        "train_generator = AVGenerator(trainfile,database_dir_path= database_dir_path, batch_size=batch_size, shuffle=True)\n",
        "val_generator = AVGenerator(valfile,database_dir_path=database_dir_path, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if NUM_GPU > 1:\n",
        "    parallel_model = model_ops.ModelMGPU(AV_model,NUM_GPU)\n",
        "    adam = optimizers.Adam()\n",
        "    loss = audio_loss(gamma=gamma_loss,beta=beta_loss,num_speaker=people_num)\n",
        "    parallel_model.compile(loss=loss,optimizer=adam)\n",
        "    print(AV_model.summary())\n",
        "    parallel_model.fit_generator(generator=train_generator,\n",
        "                            validation_data=val_generator,\n",
        "                            epochs=epochs,\n",
        "                            workers = workers,\n",
        "                            use_multiprocessing= use_multiprocessing,\n",
        "                            callbacks=[TensorBoard(log_dir='./log_AV'), checkpoint, rlr],\n",
        "                            initial_epoch=initial_epoch\n",
        "                            )\n",
        "if NUM_GPU <= 1:\n",
        "    adam = optimizers.Adam()\n",
        "    loss = audio_loss(gamma=gamma_loss,beta=beta_loss, num_speaker=people_num)\n",
        "    AV_model.compile(optimizer=adam, loss=loss)\n",
        "    print(AV_model.summary())\n",
        "    AV_model.fit_generator(generator=train_generator,\n",
        "                            validation_data=val_generator,\n",
        "                            epochs=epochs,\n",
        "                            workers = workers,\n",
        "                            use_multiprocessing= use_multiprocessing,\n",
        "                            callbacks=[TensorBoard(log_dir='./log_AV'), checkpoint, rlr],\n",
        "                            initial_epoch=initial_epoch\n",
        "                            )\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "as_0: (None, 298, 257, 2)\n",
            "as_1: (None, 298, 257, 96)\n",
            "as_2: (None, 298, 257, 96)\n",
            "as_3: (None, 298, 257, 96)\n",
            "as_4: (None, 298, 257, 96)\n",
            "as_5: (None, 298, 257, 96)\n",
            "as_6: (None, 298, 257, 96)\n",
            "as_7: (None, 298, 257, 96)\n",
            "as_8: (None, 298, 257, 96)\n",
            "as_9: (None, 298, 257, 96)\n",
            "as_10: (None, 298, 257, 96)\n",
            "as_11: (None, 298, 257, 96)\n",
            "as_12: (None, 298, 257, 96)\n",
            "as_13: (None, 298, 257, 96)\n",
            "as_14: (None, 298, 257, 96)\n",
            "as_15: (None, 298, 257, 8)\n",
            "AS_out: (None, 298, 2056)\n",
            "AVfusion: (None, 298, 2568)\n",
            "lstm: (None, 298, 1024)\n",
            "fc1: (None, 298, 1024)\n",
            "fc2: (None, 298, 1024)\n",
            "fc3: (None, 298, 1024)\n",
            "complex_mask: (None, 298, 1028)\n",
            "complex_mask_out: (None, 298, 257, 2, 2)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 298, 257, 2)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "as_conv1 (Conv2D)               (None, 298, 257, 96) 1440        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 298, 257, 96) 384         as_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv2 (Conv2D)               (None, 298, 257, 96) 64608       re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 298, 257, 96) 384         as_conv2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_2 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv3 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 298, 257, 96) 384         as_conv3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv4 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 298, 257, 96) 384         as_conv4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_4 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv5 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 298, 257, 96) 384         as_conv5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_5 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv6 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 298, 257, 96) 384         as_conv6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_6 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv7 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 298, 257, 96) 384         as_conv7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_7 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv8 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 298, 257, 96) 384         as_conv8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_8 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv9 (Conv2D)               (None, 298, 257, 96) 230496      re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 298, 257, 96) 384         as_conv9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_9 (ReLU)                  (None, 298, 257, 96) 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "as_conv10 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 298, 257, 96) 384         as_conv10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_10 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv11 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 298, 257, 96) 384         as_conv11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_11 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv12 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 298, 257, 96) 384         as_conv12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv13 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 298, 257, 96) 384         as_conv13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_13 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv14 (Conv2D)              (None, 298, 257, 96) 230496      re_lu_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 298, 257, 96) 384         as_conv14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_14 (ReLU)                 (None, 298, 257, 96) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "as_conv15 (Conv2D)              (None, 298, 257, 8)  776         re_lu_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 298, 257, 8)  32          as_conv15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 75, 1, 1792,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_15 (ReLU)                 (None, 298, 257, 8)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 75, 1, 1792)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 75, 1, 1792)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 298, 2056)    0           re_lu_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 298, 256)     4857344     lambda_2[0][0]                   \n",
            "                                                                 lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 298, 2568)    0           reshape_1[0][0]                  \n",
            "                                                                 sequential_1[1][0]               \n",
            "                                                                 sequential_1[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 298, 2568)    0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 298, 1024)    29433856    time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "fc1 (Dense)                     (None, 298, 1024)    1049600     bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "fc2 (Dense)                     (None, 298, 1024)    1049600     fc1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "fc3 (Dense)                     (None, 298, 1024)    1049600     fc2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "complex_mask (Dense)            (None, 298, 1028)    1053700     fc3[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 298, 257, 2,  0           complex_mask[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 41,331,884\n",
            "Trainable params: 41,326,108\n",
            "Non-trainable params: 5,776\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 1e-05.\n",
            "  2/328 [..............................] - ETA: 52:25 - loss: 1.1470  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (1.786230). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "328/328 [==============================] - 533s 2s/step - loss: 0.6681 - val_loss: 0.6868\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68676, saving model to /content/data/saved_AV_models/AVmodel-2p-001-0.68676.h5\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 519s 2s/step - loss: 0.5992 - val_loss: 0.5161\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68676 to 0.51606, saving model to /content/data/saved_AV_models/AVmodel-2p-002-0.51606.h5\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 515s 2s/step - loss: 0.5907 - val_loss: 0.5911\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.51606\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 522s 2s/step - loss: 0.5453 - val_loss: 0.5962\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.51606\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 1e-05.\n",
            "328/328 [==============================] - 522s 2s/step - loss: 0.5166 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.51606 to 0.44652, saving model to /content/data/saved_AV_models/AVmodel-2p-005-0.44652.h5\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 525s 2s/step - loss: 0.5047 - val_loss: 0.5668\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.44652\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 526s 2s/step - loss: 0.5026 - val_loss: 0.6011\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.44652\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 527s 2s/step - loss: 0.5000 - val_loss: 0.6354\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.44652\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 531s 2s/step - loss: 0.4982 - val_loss: 0.5658\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.44652\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 2.0000000000000003e-06.\n",
            "328/328 [==============================] - 525s 2s/step - loss: 0.4964 - val_loss: 0.5275\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.44652\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 526s 2s/step - loss: 0.4955 - val_loss: 0.5357\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.44652\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 524s 2s/step - loss: 0.4945 - val_loss: 0.4858\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.44652\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 521s 2s/step - loss: 0.4940 - val_loss: 0.4878\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.44652\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 509s 2s/step - loss: 0.4932 - val_loss: 0.5689\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.44652\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 495s 2s/step - loss: 0.4922 - val_loss: 0.5352\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.44652\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 491s 1s/step - loss: 0.4920 - val_loss: 0.3157\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.44652 to 0.31569, saving model to /content/data/saved_AV_models/AVmodel-2p-016-0.31569.h5\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 489s 1s/step - loss: 0.4910 - val_loss: 0.2222\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.31569 to 0.22215, saving model to /content/data/saved_AV_models/AVmodel-2p-017-0.22215.h5\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 490s 1s/step - loss: 0.4908 - val_loss: 0.5084\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.22215\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 489s 1s/step - loss: 0.4896 - val_loss: 0.5148\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.22215\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 494s 2s/step - loss: 0.4890 - val_loss: 0.5812\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.22215\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 505s 2s/step - loss: 0.4885 - val_loss: 0.6068\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.22215\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 516s 2s/step - loss: 0.4878 - val_loss: 0.4469\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.22215\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 519s 2s/step - loss: 0.4869 - val_loss: 0.4337\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.22215\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 528s 2s/step - loss: 0.4861 - val_loss: 0.3906\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.22215\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 523s 2s/step - loss: 0.4854 - val_loss: 0.5679\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.22215\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 497s 2s/step - loss: 0.4845 - val_loss: 0.3345\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.22215\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4838 - val_loss: 0.6279\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.22215\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 492s 2s/step - loss: 0.4825 - val_loss: 0.7592\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.22215\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 492s 1s/step - loss: 0.4817 - val_loss: 0.5687\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.22215\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4807 - val_loss: 0.3896\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.22215\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 493s 2s/step - loss: 0.4795 - val_loss: 0.6422\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.22215\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 494s 2s/step - loss: 0.4783 - val_loss: 0.6576\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.22215\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 497s 2s/step - loss: 0.4772 - val_loss: 0.4167\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.22215\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 498s 2s/step - loss: 0.4762 - val_loss: 0.4820\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.22215\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 497s 2s/step - loss: 0.4749 - val_loss: 0.4379\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.22215\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 500s 2s/step - loss: 0.4738 - val_loss: 0.4263\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.22215\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 503s 2s/step - loss: 0.4728 - val_loss: 0.6647\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.22215\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            "328/328 [==============================] - 498s 2s/step - loss: 0.4717 - val_loss: 0.5549\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.22215\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
            " 33/328 [==>...........................] - ETA: 7:20 - loss: 0.4584"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b82092457eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./log_AV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIZfEWSJkK0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}